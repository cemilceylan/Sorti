#,WS25,ProgData - Programmieren mit Daten = Intro to Deep Learning using Computer Vision examples,,,,,
#,Date,Content,File,#charts,First,Last,Remark
,,,,,,,
,,Optimierung,Do. 12:30 V Â» Labor z.B. in 207,,,,
,,,Fr. 8:00 Labor 303 Â» Vorlesung in 303,,,,
,,,Fr. 10:00 Labor 207 : Bleibt,Â» email an Frau Posselt wg. Raumplanung,,,
,,,,,,,
1,9.Okt.,PrÃ¼fung PB: Beleg mit Verteidigung â€¦,"Freitag, 6.Feb. ab 8 Uhr",Reservieren Posselt & Ramona Kunkel,,,
,,,,,,,
,,ProgData Notes or Script for Studis in WS25,"Kopiere alle meine Notizen, Videos und Colabs dort hin",,,,
,,,Invite a couple and then share via link.,,,,
,,,Link an IIb24 und IWMb23,,,,
,,,,,,,
,,,,,,,
,,The Top 15 Data Scientist Skills For 2024,https://www.datacamp.com/blog/top-15-data-scientist-skills,Ihr MS Studium,,,
,,,1.Python ... no question ;-),"NumPy, Keras, Tensorflow, ...  DM1 & 2",,,
,,,2. R  3. Statistics and Math - Statistics is not ML nor AI nor Deep Learning,MS 1.Sem. Statistic,,,
,,,"3. SQL ... Yes, certainly.  Somewhat amazing after 50 years.",BS 2.Sem. RDB,,,
,,,"5. NoSQL ... Firestore, Google File System / Hadoop Distributed File System",MS 1.Sem.: Fort. DB Konzepte 1 - Firestore ,,,
,,,,"MS 2.Sem.: Fort. DB Konzpete 2 - GFS, HDFS, ...",,,
,,,7. ML and AI ... is not Deep Learning ...,MS 1.Sem. KI / ML,,,
,,,,MS 2.Sem. Problem Solving,,,
,,,,MS 2.Sem. Mathematical Programming,,,
,,,,MS 3.Sem. Optimization and Decision Support,,,
,,,8. Deep Learning,BS 3.Sem. Programmieren mit Daten,,,
,,,,MS 2.Sem. DM1,,,
,,,,MS 3.Sem. DM2 ,,,
,,,"10.Big Data: MapReduce, BigQuery","MS 2.Sem.. Fort. DB Konzepte 2 - MapReduce, Big Query, ...",,,
,,,11. Cloud Computing: That is where it happens .. ,MS 1.Sem. & 2.Sem. & 3.Sem. ...,,,
,,,,,,,
,,History of AI / ML,,,,,
,,,Huge success with decryption during WWII. Check Enigma and Ultra secrets,,,,
,,,,Enigma machine - Wikipedia,,,
,,,One key was electro-mechanical machines to try codes until text meaningful,,,,
,,,,Bombe - Wikipedia,,,
,,,Project was secret until 1980 ...,,,,
,,,But top CS researchers knew about it.,,,,
,,,"If enigma can be decrypted, Artificial Intelligence must be possible too",,,,
,,,Logic and Neural Networks were defined when it all started at the ...,,,,
,,,....   Dartmouth workshop 1956,<<<< Read Wikipedia article in lecture,,,
,,,"We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.",,,,
,,,"Schachcomputer ... Expertensysteme zur Diagnose von Krankheiten, Reparatur von GerÃ¤ten ...",,,,
,,,"ML = manual feature selection ... many models like decision trees, Nearest Neighbour, Bayesian Classifier",,,,
,,,"""Neural Networks"" were discussed as ""of theorectical interest"" when I studied 1983-87.",,,,
,,,Deep Learning had a breakthrough in 1999 w/ MNIST by Yann LeCun,,,,
,,,"Deep Learning = Neural Networks with Hidden Layers, i.e. layers between input and output layer",,,,
,,,Deep Learning has seen tremendous progress since then,,,,
,,,Lots of data due to web / internet,,,,
,,,Lots of on-demand computer power for training in the Cloud.,,,,
,,,Discovery that Graphics Processing Units can be used to compute Neural Networks,,,,
,,,,,,,
,,Overview ,,,,,
,,History of AI / ML,,,,,
,,,Huge success with decryption during WWII. Check Enigma and Ultra secrets,,,,
,,,One key was electro mechanical machines to try codes until text meaningful,,,,
,,,"If enigma can be decrypted, Artificial Intelligence must be possible too",,,,
,,,Logic and Neural Networks were defined when it all started at the ...,,,,
,,,"""Neural Networks"" were discussed as ""of theorectical interest"" when I studied 1983-87.",,,,
,,,Deep Learning had a breakthrough in 1999 w/ MNIST by Yann LeCun,,,,
,,,Deep Learning has seen tremendous progress since then,,,,
,,,Lots of data due to web.,,,,
,,,Training in the Cloud.,,,,
,,Teachable Machines,Re-Training in the browser,,,,
,,,Image classification,,,,
,,,Landmark regression on body,,,,
,,,Audio classification,,,,
,,Classification,"Deep Learning, i.e. with hidden layers, using Fashion MNIST",,,,
,,Convolutional Neural Networks,,,,,
,,Loss Functions,Cross Entropy,,,,
,,,Claude Shannon Information Content,,,,
,,Digit Recognition with MNIST,Yann LeCun in 1989,,,,
,,,Deep CNN ValAcc = 98.39% in one hour,,,,
,,,,,,,
,,,Tensorboard to track training progress,,,,
,,Regression,,,,,
,,,Fuel Efficiency,,,,
,,,"One input, one output, i.e. regression known from school",,,,
,,,Multiple inputs,,,,
,,,Piece-wise-linear approximation,,,,
,,Activation Functions,"ReLu, SeLu, GeLu, ...",,,,
,,,x * sin(x) approximation,,,,
,,Intro to Forward and Backward calculations,,,,,
,,Quantization for on-the-edge AI,"8 bit w/ little loss in accuracy, but much faster on Neural Processing Unit (NPU), e.g. Coral TPU",,,,
,,Playground.TensorFlow.org,Regression and Classification,,,,
,,Designing Interactive AI Systems,Dancing with AI,,,,
,,Application of Regression: Landmark detection for Pose Classification,,,,,
,,Hyper Parameter,"Network Structure, e.g. number of layers",,,,
,,,"Learning and Optimization, e.g. Batch Size",,,,
,,,"Regularization, e.g. Dropout",,,,
,,"Capacity of a model, i.e. number of parameters, vs potential for overfitting",,,,,
,,Regularization,Early Stopping,,,,
,,,L1 / L2 ,,,,
,,,Dropout,,,,
,,Transfer Learning,"Intuition ""Feature Hierarchy"" in CNN",,,,
,,,"Base CNN, Feature Vector and classification head",,,,
,,,Fine tuning,,,,
,,Training Data sets,Pascal VOC,,,,
,,,ImageNet,,,,
,,,COCO,,,,
,,Object Detection,Single Shot Detector (SSD) w.g. MobileNetV2,,,,
,,Attention in Transformes for Object Detection,,,,,
,,Model Scaling for CNN,Accuracy can be achieved with ever complex models trained with ever more data.,,,,
,,,Question is how to get to best accuracy w/ limited model complexity.,,,,
,,,Scaling the parameters jointly to get best accuracy for a give model complexity,,,,
,,,EfficientNet for Classification,,,,
,,,EfficientDet for Object Detection,,,,
,,Generative AI,,,,,
,,,Autoencoder,,,,
,,,"Anomaly detection, e.g. ECG data",,,,
,,,Compression,,,,
,,Image Segmentation,Generate an image with pixels marked as an a class or instance,,,,
,,,U-Net for image segmentation of microscope images w/ biological cells,,,,
,,Variational Autoencoder,Structuring of feature space to generate image,,,,
,,,,,,,
,,Neural Style Transfer,Optimizing an image to minimze Style and Content Loss,,,,
,,,,,,,
,,Deep Dream,"Maximize certain layers, i.e. features at a certain hierarchy level, e.g. eyes.",,,,
,,,,,,,
,,Deep Convolutional Adversarial Networks,Jointly maximize a Generator and a Discriminator,,,,
,,,Generator can then generate images,,,,
,,Image Augmentation,(1) Support generalization aka avoid overfitting through generation of slightly different images,,,,
,,,"(2) Training data is ideal, e.g. face in center, which does not represent reality aka ""In-the-Wild"", e.g. face from top. Augmentation to create images that represent the in-the-wild images.",,,,
,,,E.g. rotation of faces    Training data has mostly faces in center with heads vertically aligned.,,,,
,,,"different kind of lighting - dark,   various color temperatures",,,,
,,,Distortions depending on camera positions     Training data is based on straight shots.,,,,
,,Explainable AI,Feature visualization vs Attribution,,,,
,,,Integrated Gradients to show importance of each pixel,,,,
,,End of Overview,,,,,
,,,,,,,
,,Bottom-line,"Anwendung von ""Deep Neural Networks"" inkl. CNN in Tensorflow / Keras fÃ¼r Cloud und On-the-edge",,,,
,,,,,,,
,,Motivation: Significant progress in recent years,,,,,
,,Object Detection using YOLO deep learning / neural net / Tensorflow,,,,,
,,How computers learn to recognize objects instantly | Josef Redmon,How computers learn to recognize objects instantly | Joseph Redmon,7:37,Yolo is constantly getting new versions ... ,,
,,,,,,,
,,Motivation for on-the-edge AI,Why is on-the-edge important?   Privacy ... ,,,,
,,ESP32-CAM Video Streaming and Face Recognition with Arduino IDE,ESP32-CAM Video Streaming and Face Recognition with Arduino IDE,4:19,,,
,,,,,,,
,,Das Praktische ... die Umsetzung ... was machen ... the Doing ... Die Erfahrung der Selbstwirksamkeit,,,,,
,,Google Techable Machine,Start hands-on,,,,
,,Intro web page for Teachable Machines,https://teachablemachine.withgoogle.com/,,,,
,,Follow web page ...,,,,,
,,Teachable Machine 2.0: Making AI easier for everyone,Teachable Machine 2.0: Making AI easier for everyone,0:02:08,,,
,,Teachable Machine Tutorial 1: Gather,Teachable Machine Tutorial 1: Gather,0:02:15,,,
,,Teachable Machine Tutorial 2: Train,Teachable Machine Tutorial 2: Train - YouTube,0:00:54,,,
,,Teachable Machine Tutorial 3: Export,Teachable Machine Tutorial 3: Export,0:00:59,,,
,,,,,,,
,,YOU: Try different kind of projects,Presentation & Discussion at start of next lecture,,,,
,,Image,e.g. recognize different people incl. background,,,,
,,Get one project done in 30 minutes??? ,e.g. number of faces in image,,,,
,,,,,,,
,,"Demo Image Classification: SchlÃ¼ssel, Handy, Mich als Background",,,,,
,,,,,,,
,,,,,,,
2,17.Okt,,,,,,
,,,,,,,
,,"Projekt: SchlÃ¼sselbund, Portfolio",https://teachablemachine.withgoogle.com/train/image/1m6ulaHj7ilyeePkEcqYcUL3YZxTPVyND,,,,
,,,Load project from Google drive,,,,
,,,Train model again,,,,
,,,Check under the hood: Accuracy per class: All 100% TOP! ?,,,,
,,,Hand pretending to hold Â» Keys or Portfolio,,,,
,,,What can we do to fix this problem?,,,,
,,,,,,,
,home,Teachable machine 1 : Image Classification,Teachable Machine 1: Image Classification,0:20:01,,,
,home,Teachable Machine 2: Snake Game,Teachable Machine 2: Snake Game,0:07:58,,,
,home,Teachable Machine 3: Sound Classification,Teachable Machine 3: Sound Classifiication,0:14,,,
,,,,,,,
,,"Demo Posen mit up, down, left and right",Store training data and model in GDrive for Dancing with AI,,,,
,,,https://teachablemachine.withgoogle.com/train/pose,,,,
,,,Save project,,,,
,,,Enter model URL here,https://teachablemachine.withgoogle.com/models/44M85Ue9C/,,,
,,,,,,,
,,,,,,,
,,Image,"SchlÃ¼sselbund, Handy, Mich als Background",,,,
,,Pose Project,"HÃ¤nde machen Pfeil: Up, left, right and down",,,,
,,Sound,"e.g. Hello, Data Mining, Peter & background noise",,,,
,,,https://teachablemachine.withgoogle.com/train/pose,,,,
,,,,,,,
,,YOU: Try different kind of projects,Im Labor ...,,,,
,,,,,,,
,,AIforAnyone 1: AI and the Gartner Hype Cycle,AI and the Gartner Hype Cycle,0:04:48,,,
,,,,,,,
,,KI = Prog. mit Daten,,,,,
,,,,,,,
,,Sorting Marshmallows with AI: Using Coral + Teachable Machine,Sorting Marshmallows with AI: Using Coral + Teachable Machine,0:02:12,,,
,,Teachable Sorter,Teachable Sorter | Coral,,,,
,,,Computer Vision details: Global vs Rolling Shutter,Draw on blackboard. ,,,
,,,,Camera behind a propellor needs to use a global shutter ...,,,
,,Tiny Sorter as a project on-the-edge,Tiny Sorter,,,,
,,,,,,,
,,,Sie bauen das im Labor!,,,,
,,,,,,,
,,Ihre Projekte mit Teachable Machines?,,,,,
,,Pose Project,"e.g. Hand signals, stop, right, left, above, below & background",,,,
,,,,,,,
,,Raspberry Pi & Teachable Machine â€“ Tensorflow Modelle fÃ¼r KI nutzen,Raspberry Pi & Teachable Machine - Tensorflow Modelle fÃ¼r KI nutzen,,,,
,,,,,,,
,,Designing Interactive AI Systems,Dancing with AI,,,,
,,,Open Project with balls following fingers Â» Demo,,,,
,,,https://playground.raise.mit.edu/create/?project=https://dancingwithai.media.mit.edu/projects/circle-drawing.sb3,,,,
,,,Explain thread for yellow and green ball,,,,
,,,Discuss Hand Sensing functions / blocks,In case camera image is not shown Â» Restart Pixelbook,,,
,,,,,,,
,,,Open Project with moving person on stage with index finger ,,,,
,,,https://playground.raise.mit.edu/create/?project=https://dancingwithai.media.mit.edu/projects/FingerPuppetShow.sb3,,,,
,,,Explain code in one thread,,,,
,,,Move around ,,,,
,,,Again based on hand sensing function,,,,
,,,,,,,
,,,Open project with bird open mouth,,,,
,,,https://playground.raise.mit.edu/create/?project=https://dancingwithai.media.mit.edu/projects/ChirpBirdForever.sb3,,,,
,,,Explain two threads,,,,
,,,Open mouth to make bird chirp,,,,
,,,Discuss face sensing blocks,,,,
,,,"Change from ""open mouth"" to ""lip stretch""",,,,
,,,"Change to ""<feeling (joyfull)>""",,,,
,,,,,,,
,,,,,,,
3,24.Okt.,Wahl der individuellen Vertiefungen,IIb24 - Wahl in WS25 und SS26 fÃ¼r indivi. Vertief. ab SS26 - 2025-10-22,,,,
,,,,,,,
,,Wiederholung,,,,,
,,Tiny Sorter as a project on-the-edge,Tiny Sorter,,,,
,,,Projekt fÃ¼r Ihren Beleg?,,,,
,,,,,,,
,,,Open Project with lady holding hand in-front of face,,,,
,,,https://playground.raise.mit.edu/create/?project=https://dancingwithai.media.mit.edu/projects/NewShyDog.sb3,,,,
,,,Explain ability to upload your model from Teachable Machine,,,,
,,,Explain Teachable Machine blocks / functions,,,,
,,,Train model to make dog stop or move,,,,
,,,Upload model in Teachable Machines,,,,
,,,Enter sharable link into Dancing with AI Scratch project,,,,
,,,,,,,
,,,Click on lower left corner to see library of extensions,,,,
,,,Upload Teachable Machine model,,,,
,,,Hand Sensing,,,,
,,,Face Sensing,,,,
,,,Body Sensing,,,,
,,,,,,,
,,,Open project with snake moving,,,,
,,,https://playground.raise.mit.edu/create/?project=https://dancingwithai.media.mit.edu/projects/SnakeGame.sb3,,,,
,,,"Train an image or pose model for up, down, left and right",,,,
,,,Insert URL,https://teachablemachine.withgoogle.com/models/44M85Ue9C/,,,
,,,Try ,,,,
,,,,,,,
,,,Ausprobieren zuhause und im Labor,,,,
,,,,,,,
,,Forschung Â» Neue Einsichten Â» VerÃ¶ffentlichungen,https://scholar.google.com/,,,,
,,,Wenn viele andere Forscher Ihre VerÃ¶ffentlichungen zitieren ist sie richtig und wichtig.,,,,
,,Angewandte Forschung / Entwicklung Â» Patente,https://patents.google.com/ ,,,,
,,,Innovative LÃ¶sung eines realen Problems.,,,,
,,,"""Reduction to Practice"" so that somebody trained in the arts can build your solution.",,,,
,,,Publish your work to support progress in exchange for government protection.,,,,
,,,Ohne Patente gewinnen diejenigen mit den geringsten Produktionskosten.,,,,
,,,,,,,
,,Projects at end of ProgData,,,,,
,,,Face recognition for coffee machine: Re-train face detector,,,,
,,,Tiny Sorter,,,,
,,,Gesture control of game,,,,
,,,Emotion detection in speech,,,,
,,,Emotion detection in face,,,,
,,,,,,,
,,Gesture control of game,,,,,
,,,Train model to recognize gestures,,,,
,,,Use model to create keystrokes,,,,
,,,Control game,,,,
,,,,,,,
,,DM2 Belege,Es lohnt sich ... halten Sie durch!,,,,
,,,Anton KieÃŸling - DM2-PrÃ¤senzsensor- 2025-02-07.pdf,,,,
,,,Tobias Frenzel - Data Mining 2 Beleg 2025-02-24.pdf,,,,
,,,,,,,
,,Building a Speech Emotion Recognizer using Python,Building a Speech Emotion Analyzer in Python | by Soukaina Alaoui | ð€ðˆ ð¦ð¨ð§ð¤ð¬.ð¢ð¨ | Medium,,,,
,,,,,,,
,,,,,,,
,@home,"But what is a Neural Network? | Deep Learning, chapter 1",But what is a neural network? | Deep learning chapter 1,0:19:13,Excellent NN intro video with a connection to the example,,
,,,,,"Showing DNN, not CNN.",,
,,,,,,,
,,"""Hello World"" of Deep Learning","The ""Hello, World"" of ML codelab",,,,
,,,Hello-ML-World - Copy for Studis WS25 - KtH 2025-10-24.ipynb,,,,
,,,Lab2-Computer-Vision - Copy for Studis WS25 - KtH 2025-03-12.ipynb,,,,
,,Some students have a Gaming Rig with a powerful nVidia GPU Â»,https://www.tensorflow.org/install/pip,,,,
,,,Hello-ML-World - KtH 2024-03-17.ipynb,,,,
,,,That was your first regression model.,,,,
,,,Did we build regression models before?,Pose estimation is based on regression.,,,
,,,,,,,
,,"Deep Learning with Hidden Layers using ""Fashion MNIST""",Build a computer vision model with Tensorflow,Sketch neural network on blackboard,,,
,,,GitHub - zalandoresearch/fashion-mnist: A MNIST-like fashion product database. Benchmark,Zalando Research ... Berlin.,,,
,,,,,,,
4,7.Nov.,Wahl der individuellen Vertiefungen,IIb24 - Wahl in WS25 und SS26 fÃ¼r indivi. Vertief. ab SS26 - 2025-10-22,,,,
,,,,,,,
,,Repetition and Restart,,,,,
,,"Deep Learning with Hidden Layers using ""Fashion MNIST""",Build a computer vision model with Tensorflow,Re-start: Sketch model on blackboard,,,
,,,GitHub - zalandoresearch/fashion-mnist: A MNIST-like fashion product database. Benchmark,Zalando Research ... Berlin.,,,
,,,Lab2-Computer-Vision - KtH 2025-11-05.ipynb,Number of parameters in each layer,,,
,,,,Exercises,,,
,,,,,,,
,,Axioms of Probability defined by Kolmogorov,Probability | Axioms | Chance | Likelihood,,,,
,,Softmax Activation Function â€” How It Actually Works,Softmax Activation Function â€” How It Actually Works | TDS Archive,,,,
,,,Softmax Activation Function in Python: A Complete Guide | DataCamp,,,"use both articles, since the 2nd one shows how to use in Keras / Tensorflow",
,,,,,,,
5,14.Nov.,Wahl der individuellen Vertiefungen,IIb24 - Wahl in WS25 und SS26 fÃ¼r indivi. Vertief. ab SS26 - 2025-10-22,,,,
,,,,,,,
,,Fragen,24 Studis im Matrikel vs Anwesende in V und Lab?,,,,
,,,Zu spÃ¤t kommen? Bus? Â» Vorlesung spÃ¤ter anfangen?,,8:15,,
,,"Fragen Sie bitte, wenn Sie Fragen haben. ;-)",,,,,
,,,,,,,
,,"Deep Learning with Hidden Layers using ""Fashion MNIST""",Lab2-Computer-Vision - KtH 2025-10-24.ipynb,"Create the 64, 32, 16 model and show it trains faster and performs better on unseen data",,,
,,,,Exercises,,,
,,,,,,,
,,Draw exp and log functions on black board,Discuss use of exp for softmax,,,,
,,,... use of log for Cross Entropy Loss function,,,,
,,,,,,,
,,Cross-Entropy Loss Function,A Brief Overview of Cross Entropy Loss | by Chris Hughes | Medium,,,,
,,,Cross-Entropy Loss Function in Machine Learning: Enhancing Model Accuracy | DataCamp,,,,
,,,,Understanding this helps explain why data quality often matters more than model architecture.,,,
,,,,Teacher and curriculum make all the difference ... ;-),,,
,,Information Content,Information content - Wikipedia,,,,
,,Claude Shannon,Claude Shannon - Wikipedia,,,,
,,,,,,,
,,ML on edge devices: Intro vids,,,,,
,,Intro to On-device Machine Learning (TF 2020 Updates),Intro to On-device Machine Learning (TF Fall 2020 Updates),0:11:45,,,
,,,Summary ,,,,
,,,"Model architecture, e.g. MobileNetV3",0:07:38,,,
,,,Accuracy vs Model Size vs Runtime,,,,
,,,Quantization,,,,
,,,Pruning,,,,
,,,On-the-edge TPU,,,,
,,,,,,,
,,Tensorflow Lite examples,https://www.tensorflow.org/lite/examples,,,,
,,Image classification,https://www.tensorflow.org/lite/examples/image_classification/overview,,,,
,,Image classification,https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/raspberry_pi,,,,
,,Optical character recognition (OCR),https://github.com/tensorflow/examples/tree/master/lite/examples/optical_character_recognition/android,,,,
,,,,,,,
,,Warning,"The models are not general enough for OCR in the wild (say, random images taken by a smartphone camera in a low lighting condition).",,,,
,,,,,,,
,,Projekt? Autokennzeichen auf der StraÃŸe?,,,,,
,,BERT Question and Answer (Tensorflow Lite),https://www.tensorflow.org/lite/examples/bert_qa/overview,,,,
,,Projekt? Chat mit RPI?,,,,,
,,,,,,,
,,Convolutional Neural Networks (CNN),,,,,
,,,,,,,
,,Kernel (image processing),https://en.wikipedia.org/wiki/Kernel_(image_processing),,,,
,,,Manually created Kernels,,,,
,,,Show animation,,,,
,,,,,,,
,,Sobel Kernel Â» edge detection ,Sobel operator - Wikipedia,From 1968 ~ Invention of C and UNIX,,,
,,Application: Autofocus based on contrast,Autofocus using OpenCV: A Comparative Study of Focus Measures for Sharpness Assessment,,,,
,,,Show video to illustrate AutoFocus using contrast,,,,
,,,Entropy-based Focus Measure ... ,Blurry Â» all the same Â» little chaos Â» little information content,,,
,,,Tenengrad (Sobel Gradient),,,,
,,,,,,,
6,21.Nov.,Start: 8:15,,,,,
,,,,,,,
,,NÃ¤chste Woche: Keine LV wg. SG / TSG WS mit Rektor,,,,,
,,,,,,,
,,Laboratory,Competition to increase efficiency TestAcc / #parameters,,,,
,,,"Winning architecture 1, 2, 10 nodes",,,,
,,,The relevant information about an entire  28x28x1 images was reduced to one number.,,,,
,,,Question: How much information can you put into one floating point number?,,,,
,,,Unendlich viel?,,,,
,,,Quantisierung,,,,
,,,Rauschen,,,,
,,,Will encode images to a couple of numbers using an AutoEncoder later.,,,,
,,,,,,,
,,Build convolutions and perform pooling,Build convolutions and perform pooling | Google for Developers,,,,
,,Lab3-What-Are-Convolutions - KtH 2024-03-17.ipynb,https://colab.research.google.com/drive/1XtOL1pLTxGT_sWoy9M1RKIE_z0xGOjGR,,,,
,,Bottom of Colab window,Variables,,,,
,,,Terminal into Linux on Virtual Machine,,,,
,,,Gemini button,,,,
,,,,,,,
,,Build convolutional neural networks (CNNs) to enhance computer vision,https://developers.google.com/codelabs/tensorflow-4-cnns#0,"Sketch neural networks on blackboard w/ ""images"" and kernels at various layers",,,
,,Lab4-Using-Convolutions - KtH 2024-03-17.ipynb,https://colab.research.google.com/drive/1CVSUn2b5adX49aXnJJJJBze803b2zUdb,Sketch DNN network and compute number of parameters,,,
,,,,Sketch CNN network and compute number of parameter,,,
,,,,,,,
,Lab. 21.Nov.,Wettbewerb im Labor,,,,,
,,,Lab4-Using-Convolutions - KtH 2025-03-128.ipynb,,CNN vs DNN,,
,,,,,CNN Architektur,,
,,,,,,,
,,,WettkÃ¤mpfe,,Andere Wettbewerbe,,
,,,A) Maximale Effizienz,,Maximale Genauigkeit Â» GroÃŸe Modelle mit viel Training Â» Weniger geeignet fÃ¼r das Labor,,
,,,B) (1) > 85% und (2) Maximaler Effizienz Quotient,,,,
,,,,,,,
,,Name,Architecture,Trainable,Accuracy,,Effizienz
,,,CNN vs DNN | Architecture: Nodes or kernels per layer,Parameter,Training,Test,Test Acc / #Parameter * 1000
,,Lab4,"DNN 64, 10",50890,88.60%,86.26%,17
,,Lab4,"CNN 16, 32, 64 | DNN 128, 10",32906,88.40%,87.16%,26
,,,6 - 10 Epochs,32906,91.30%,88.96%,27
,,,11-20 Epochs,0:00:00,93.02%,88.62%,27
,,Lab4 - MaxEff,"CNN 8, 16, 32 | DNN 32, 10",7274,85.61%,84.59%,116
,,,6 - 10 Epochs,7274,87.69%,86.65%,119
,,,11-15 Epochs,7274,88.58%,86.82%,119
,,,16-20 Epochs,7274,89.29%,86.79%,Overfitting
,,Lab4 - MaxEff II,"CNN 8, 16, 32 | DNN 10",6218,85.20%,85.03%,137
,,,6 - 10 Epochs,6218,87.78%,85.83%,138
,,Lab4 - MaxEff III,,,,,
,,,,,,,
,,,,,,,
,,Korber,"CNN 16, 16, 8 | DNN 64, 10",4866,88.09%,86.46%,178
,,Korber II,"CNN 16, 8, 8 | DNN 64, 10",3130,85.86%,85.20%,272
,,,,3130,86.29%,84.04%,Overfitting
,,,"CNN 16, 4, 8 | DNN 32, 10",1654,79.78%,79.95%,483
,,,,,,,
,,Lehmann,"CNN 6, 10, 20 | DNN 10, 10",2750,80%,83.00%,302
,,Haupt,"CNN 32, 64 | DNN 128, 10",225034,99.05%,99.39%,Super gut!
,,,,,,,
,,Bergmann,"1, 2, 10",819,49,48,59
,,Bergmann,"6, 8, 10",4856,85.8,85,18
,,,,,,,
7,5.Dez.,8:15,,,,,
,,,,,,,
,,Note basierend auf einem Beleg,In drei Wochen ist Weihnachten. Noch vier Wochen im Januar bis zur Verteidigung.,,,,
,,,Bitte bilden Sie Gruppen und Ã¼berlegen sich Projekte,,,,
,,,"Sie bekommen 5 ECTS, d.h. der Beleg hat ~5 Seiten pro Teilnehmer",,,,
,,Ideen,"Bitte im tab: ""Teams und Projekte"" editieren",,,,
,,,,,,,
,,Gruppe 1,Sortieren von Ã„pfelsorten,,,,
,,Vorname,Nachname,,,,
,,Giang,Tran,,,,
,,Jonas,Hebenstreit,,,,
,,Gruppe 2,,,,,
,,Robin,Haase,,,,
,,Paul,Glaus,,,,
,,Gruppe 3,,,,,
,,Alex Robert,Bergmann,,,,
,,Alexander,Szczuka,,,,
,,Gruppe 4,,,,,
,,Felix,Lehmann,,,,
,,Nicolas,Kirsche,,,,
,,Gruppe 5,,,,,
,,Amon,Haupt,,,,
,,Vincent,Klamert,,,,
,,Gruppe 6,,,,,
,,Lorenz,Korber,,,,
,,Timo,MÃ¼ller,,,,
,,Gruppe 7,,,,,
,,Richard,WÃ¶lflick,,,,
,,Friedrich,Kayser,,,,
,,Gruppe 8,,,,,
,,Martin,Leipert,,,,
,,Nishan,Singh,,,,
,,Gruppe 9,,,,,
,,Mouad,Bakkali,,,,
,,Ben,Holub,,,,
,,Gruppe 10,,,,,
,,Luca,Hiller,,,,
,,Simon,Schulzensohn,,,,
,,Sandra,Mayer,,,,
,,Gruppe 11,,,,,
,,Alexander,Bergmann,,,,
,,Marc-Louis,MÃ¼nnich,,,,
,,Charlie-Vincent ,Lucke,,,,
,,,,,,,
,,ToDo,Build teams,,,,
,,,Start consultations during lab and lecture time to,,,,
,,,Define a project,,,,
,,,Work on project,,,,
,,,Create a presentation for the defense,,,,
,,,Write a report,,,,
,,,Submit report,,,,
,,,,,,,
,,Results from Lab 21.Nov.,Haupt: 99.39% TestAccuracy,Students can get incredible good with sufficient data to train,,,
,,,,"The ""model architecture"" race is about efficiency not accuracy",,,
,,,Korber: 272 Efficiency ,There is tremendous potential to increase efficiency,,,
,,,Korber CNN 272 Efficiency vs 59 Bergmann DNN (14.Nov.),CNN offers 5x more efficiency,,,
,,,,,,,
,@home,Visualizing CNN | Layer by Layer,Visualizing Convolutional Neural Networks | Layer by Layer,0:05:52,,,
,,,,,,,
,,Use convolutional neural networks (CNNs) with complex images,https://developers.google.com/codelabs/tensorflow-5-compleximages#0,Sketch neural networks on blackboard,,,
,,Lab5-Using-Convolutions-With-Complex-Images - KtH 2024-03-17.ipynb,Lab5-Using-Convolutions-With-Complex-Images - KtH 2024-03-22.ipynb,Compute number of parameters,,,Please note: 40 Million parameters
,,,Instead of 28x28x1 images of horses and humans w/ 300x300x3,,,,
,,,500 horse images and 527 humans,Please note overfitting,,,
,,,Training acc. 98% but val_acc. 78%,,,,
,,,,,,,
,skip ,Use convolutional neural networks (CNNs) with large datasets to avoid overfitting,https://developers.google.com/codelabs/tensorflow-6-largecnns#0,Sketch neural networks on blackboard,,,
,>>>,Lab6-Cats-v-Dogs - KtH 2024-03-17.ipynb,Lab6-Cats-v-Dogs - KtH 2025-04-03.ipynb,Compute number of parameters,,,
,,,22498 images for training,,,,
,,,2500 images for validation,Discuss importance of shuffling to avoid files from one class in one batch,,,
,,,images are (150x150x3),batch_size and steps_per_epoch,,,
,,,train_acc: 92%   val_acc: 83%,More data does not avoid overfitting,,,
,,,,"But it forces generalization, which enables more accuracy on unknown data.",,,
,,,,,,,
,,How Deep Learning started ...,... after being discussed as a cool concept sind 1958.,,,,
,,MNIST Yann LeCun,MNIST database - Wikipedia,,,,
,,,History ... 1989 LeNet from Yann LeCun,,,,
,,,LeNet - Wikipedia,9 years from LeNet-1 to LeNet-5 ...,,,
,,,Section: Classifiers and results,,,,
,,,"Traditional ML e.g. lin. classifier, K-NN, SVM works too, but recent progress in DNN.",,,,
,,,CNN w/ various configurations,,,,
,,,Winner: Ensemble of CNN,,,,
,,,.... and data augmentation with translation and rotation,,,,
,,LeNet by Yann LeCun on MNIST,LeNet - Wikipedia,,,,
,,,1988 - 1998,,,,
,,,Optimized a CNN using backpropagation,,,,
,,,"Kernel was not always 3x3, e.g. 5x5, 11x11",,,,
,,AlexNet by Alex Krizhevsky on ImageNet,AlexNet - Wikipedia,,,,
,,,2012,,,,
,,,,,,,
,,,ImageNet w/ 1.2 million images,,,,
,,,The rest ist history ... ;-),,,,
,,Colab: DNN and CNN to classify MNIST digits in less than 30 lines - KtH 2022-04-07,DNN and CNN to classify MNIST digits in less than 30 lines - KtH 2025-03-28.ipynb,,,,
,,Results,DNN Â» TestAcc = 98%,,,,
,,,CNN  shallow Â» TestAcc = 98.39%,,,,
,,,Present Deep CNN,,,,
,,Colab: DNN and CNN to classify MNIST digits in less than 30 lines - KtH 2022-04-07,DNN and CNN to classify MNIST digits in less than 30 lines - KtH 2025-03-28.ipynb,,,,
,,DNN TestAcc = 97%   CNN TestAcc = 98%,Deep CNN TestAcc = 99.15%,,,,
,,,Hierarchy of features works,,,,
,,About the same number of parameters!,Human intelligence still matters in building better AI.,,,,
,,,,,,,
,,"Stay curious! Check-out: Network structure, parameter of layers, e.g. number of nodes, optimizer, activation functions",,,,,
,,"Selecting a better optimizers would help, since the training accuracy is very high already AND the optimizers is driving the training accuracy only!",,,,,
,,,,,,,
8,19.Dez.,"Teams, Projekte Â» Konsultationen im Januar",ProgData Notes or Script for Studis in WS25,Weitere Diskussionen im Labor,,,
,,,,,,,
,>>> ,Tensorboard_in_notebooks - KtH 2022-04-07.ipynb,Tensorboard_in_notebooks - KtH 2022-04-07.ipynb,Show increasing training accuracy with stagnating validation accuracy.,,,
,,,,,,,
,Not SS22,Debugging and Monitoring TensorFlow Programs,https://www.pluralsight.com/courses/tensorflow-programs-debugging-monitoring,Great intro to debugging TensorFlow Graph with tfdbg aka TensorFlow Debugger,,,
,Not SS23,Course page,https://app.pluralsight.com/course-player?clipId=01445d93-6510-4327-aa97-5c4d22db847a,Explains the TensforFlow Graph shown by Tensorboard,,,
,,,,,,,
,>>>,AIforAnyone 5: Climbing up the slope of enlightment in AI and ML,https://www.youtube.com/watch?v=ig--SbOm4mQ,0:04:01,,,
,,,,,,,
,,So far focus on classification. Now moving to regression and to study the power of modelling using DNN.,,,,,
,,"Regression is useful by itself to deliver predictions on ""analog"" values and ...",,,,,
,,... Object detection is classification of each of the objects plus regression of the bounding box coordinates.,,,,,
,,"Finally Regression is a great way to show the fundamental working of Deep Learning, since ...",,,,,
,,"... fundamentally that is what a neural network does. Classification is associating an ""analog"" value to a class probability.",,,,,
,,,,,,,
,,Classification vs Regression,Regression is really the base of Neural Networks,,,,
,,,,,,,
,>>>>,Get started with using Tensorflow to solve for regression problems (Coding TensorFlow),https://www.youtube.com/watch?v=-vHQub0NXI4,0:11:38,,,
,,,,,,,
,>>>>,Regression to predict fuel efficiency - KtH 2021-04-02.ipynb,Regression to predict fuel efficiency - KtH 2024-04-05.ipynb,,,,
,,,,,,,
,,Intuition of pattern recognition and combining them to get to more efficient ,,,,,
,Shown already,"But what is a Neural Network? | Deep Learning, chapter 1",But what is a neural network? | Deep learning chapter 1,0:19:13,Excellent NN intro video with a connection to the example,,
,@home,"Gradient descent, how neural networks learn | Deep learning, chapter 2","Gradient descent, how neural networks learn | Chapter 2, Deep learning - YouTube",0:21:00,,,
,,,,,,,
,,Hyperparameter: Activation functions,"ReLu, SeLu or GELU?",,,,
,New in SS25,How to choose the right activation functions for Neural Networks,https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c,,,,
,New in SS25,What happens if you do not use any activiation function in a neural network's hidden layers?,https://medium.com/data-science-365/what-happens-if-you-do-not-use-any-activation-function-in-a-neural-networks-hidden-layer-s-f3ce089e4508,,,,
,>>>,GeLU in Tensorflow,tf.keras.activations.gelu | TensorFlow v2.16.1,,,,
,>>>,"Is GELU, the ReLU successor?",https://towardsai.net/p/l/is-gelu-the-relu-successor,,,,
,>>>,Colab: Activation Functions,Activation_Functions - KtH 2025-11-19.ipynb,Write python to create a network,,,
,,,,Activation functions support optimization differently.,,,
,,,,The best activation functions enable the best training,,,
,,,The activation function is one hyper parameter,,,,
,,,A lot of research into finding better activation functions,,,,
,,,,,,,
9,9.Jan.,"3 Lectures left: 9., 16. & 23.",Labor: Discuss of your projects,,,,
,,"Teams, Projekte Â» Konsultationen im Januar",ProgData Notes or Script for Studis in WS25,Weitere Diskussionen im Labor,,,
,,,,,,,
,,Innovation durch Forschung,Ohne Innovation ... Kein Wachstum oder mehr arbeiten!,,,,
,>>>,"Is GELU, the ReLU successor?",https://towardsai.net/p/l/is-gelu-the-relu-successor,,,,
,,,,,,,
,,Forschung,"Dann dringen Sie in das Unbekannte vor, fallen in LÃ¶cher, lernen Neues und verÃ¶ffentlichen Ihre Erkenntnisse.",,,,
,,Erfolg in der Forschung: VerÃ¶ffentlichungen,Andere validieren Ihre Erkenntnisse und zitieren sie.,,,,
,,,"Referenzen ""Citations"" ist das MaÃŸ von Erfolg in der Forschung.",,,,
,,,Scholar.Google.com,,,,
,,,"KtH, Stefan Kornhuber, JL, Markus Ullrich, Markus Fulland (M),  Thomas Wiegert (N), Jens Weber (N), Raj Kollmorgen (S)... Sie schauen selber nach.",,,,
,,,"Unterschied HTW und Uni? Gerhard Fettweis, Heinrich Meyr, ... Christian Drosten",,,,
,,Patente,Patent is reducing an idea to practice that solves a relevant problem,,,,
,,,Warum schÃ¼tzt der Staat Geistiges Eigentum durch Patente? ,,,,
,,,Weil es so einfach ist etwas nachzubauen und so mÃ¼hselig etwas zur Praxisreife zu entwickeln. ,,,,
,,,Beispiel: Autonomes Fahren ... sollte schon seit Jahren fertig sein.,,,,
,,,Der mit den geringsten Produktionskosten gewinnt.,,,,
,,,Der Staat mÃ¶chte Innovation fÃ¶rdern und vergibt Patente.,,,,
,,,First patent in 500 BCE for a dish in Greece. First well documented in 1415 in Florence for a system to turn wool into felt.,,,,
,,,Patents.google.com,,,,
,,,,,,,
,>>>,x * sin(x) with noise: Nonlinear-regression - KtH 2021-04-08.ipynb,x * sin(x) with noise: Nonlinear-regression - KtH 2024-04-07.ipynb, Â» Better results? Much faster convergence in SS23 ...,,,
,,,,,,,
,,Model to generate data,,,,,
,,Add noise,,,,,
,,Learn a DNN model based on the data to inter- or extrapolate to unknown data,,,,,
,,Â» study impact of ,,,,,
,,,Amount of noise in training and test data,,,,
,,,Capacity of model trained,,,,
,,,Architecture of model,,,,
,,,"Hyperparameter, e.g. activation functions, batch size",,,,
,,,Duration of training,,,,
,,,"Complexity of underlying models, i.e. more or less periods",,,,
,,,,,,,
,Labor,Konsultationen mit allen Teams,,,,,
,Labor,Experimente mit x * sin(x),,,,,
,,,Lab4-Using-Convolutions - KtH 2025-03-128.ipynb,,,,
,,,Lab5-Using-Convolutions-With-Complex-Images - KtH 2024-03-22.ipynb,,"Besser fÃ¼r CNN, aber langsamer ...",,
,,,Lab6-Cats-v-Dogs - KtH 2025-11-19.ipynb,,"siehe oben, nur extremer.",,
,,,,,,,
,,KomplexitÃ¤t der Aufgabe,Mehr Auf- und AbschwÃ¼nge,,,,
,,KapazitÃ¤t des Modells,"Mehr Knoten per Ebene, mehr Ebenen Â» Mehr Parameter",,,,
,,,Â» Kann komplexe ZusammenhÃ¤nge modellieren,,,,
,,,Â» Braucht mehr Trainingsdaten um die vielen Parameter zu bestimmen,,,,
,,,"Â» GrÃ¶ÃŸeres Risiko fÃ¼r Overfitting, d.h. immer besser auf den Trainingsdaten, aber NICHT auf den Testdaten.",,,,
,,Rauschen in den Trainingsdaten,Fehler im Labeling,,,,
,,Architektur des Modells CNN vs DNN,Labor: Die richtige Architektur erhÃ¶ht die LeistungsfÃ¤higkeit und Effizienz,,,,
,,"Hyperparameter des Modells, z.B. Activation Function",,,,,
,,Umfang der Trainingsdaten,"Â» Richtige Auswahl fÃ¼r das Problem, DiversitÃ¤t, QualitÃ¤t ",,,,
,,,"Â» MÃ¼hselig, aber Programmieren mit Daten!",,,,
,,Trainingsdauer,,,,,
,,,,,,,
,,KomplexitÃ¤t der Aufgabe Â» GrÃ¶ÃŸere KapazitÃ¤t der Modells Â» Mehr Trainingsdaten,Experimente ... ,,,,
,,Mehr Fehler im Labeling Â» Rauschen Â» mehr Trainingsdaten?,,,,,
,,,,,,,
,,,,,,,
,,,,,,,
,zuhause,"What is backpropagation really doing? | Deep learning, chapter 3",https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3,0:13:53,,,
,Not in SS24,"Backpropagation calculus | Deep learning, chapter 4",https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4,0:10:17,,,
,,I won't programm an Optimizer ever. So why do I care about Backpropagation?,Memory required ~ number of parameters * BatchSize  because of Backpropagation,,,,
,Not in SS23,Backpropagation in Convolutional Neural Networks from Scratch,https://www.youtube.com/watch?v=z9hJzduHToc,0:09:20,Need to talk at least once about backpropagation to optimize the weights and biases,,
,,,,,,,
,,,,,,,
,,Intermezzo: First glance at Quantization,"Speed needs accelerator HW like edge TPU, to make this small and cheap and with reduced power consumption, TPU needs fixed point int8 Â» Replace double by int8, i.e. Quantization",,,,
,,Quantisation to run on-the-edge TPU,,,,,
,,Tensorflow Quantization Spec,https://www.tensorflow.org/lite/performance/quantization_spec,Tafel: Verteilung einer realen GrÃ¶ÃŸe Â» Normierung Â» Quantisierung,,,
,,,Quantisation needs a representative dataset to compute scale and bias to map to int8 w/ -127 to 128,,,,
,,,"BTW TFlite model was 19 kByte with quant. it is only 7 kByte, i.e. a reduction by 2.7",,,,
,,Play,"64 bit = 8 bytes vs. a byte for scale, zeroPoint and value 8/3 = 2.7",SS23:  model_no_quant.tflite 269 kByte and model_int8-quant.tflite 71kByte. 269 /71 = 3.8. Â» quant_int8 is much smaller! ,,,
,,,;-),Even a bit more than what quantization from 64 bit float to int8 with zero_point and scale would suggest.,,,
,,Run this model at an edge TPU?,,,,,
,,Get started with the USB Accelerator,https://coral.ai/docs/accelerator/get-started/,,,,
,,Edge TPU inferencing overview,https://coral.ai/docs/edgetpu/inference,,,,
,,TensorFlow models on the Edge TPU,https://coral.ai/docs/edgetpu/models-intro/,,,,
,,Run inference on the Edge TPU with Python,https://coral.ai/docs/edgetpu/tflite-python,,,,
,,Code examples and project tutorials to build intelligent devices with Coral,https://coral.ai/examples,,,,
,,,,,,,
,,Back to the main theme:,,,,,
,,Why Neural Networks can learn almost anything,https://www.youtube.com/watch?v=0QczhVg5HaI,0:10:29,,,
,,Playground.TensorFlow.org,https://playground.tensorflow.org/,,,,
,,Demo,Demo,,,,
,,Classification,,,,,
,,,Learning Rate,0.03,,,
,,,Activation,RelLu,,,
,,,,,,,
,,,Ring Model,,,,
,,,Reduce input features,X1 and X2,,,
,,,"Nodes in hidden layer : Start w/ 2 Â» not working, 3 works, since that's required for a triangle",,,,
,,,"... more nodes, more layers Â» smoother outline",,,,
,,,Select other input features,"adding sin(X1) confuses model, adding sin(X2) fixes the problem Â» feature selection",,,
,,,Reduce nodes in first hidden layer,3 nodes are sufficient,,,
,,,Reduce nodes in second layer,1 node is sufficient,,,
,,,,,,,
,,,Quadrant Model,,,,
,,,1 or 2 neurons in 2nd layer,,,,
,,,...,,,,
,,,,,,,
,,Playground.TensorFlow.org,https://playground.tensorflow.org/,,,,
,,Classification,,,,,
,,Two clusters ,,,,,
,,,"X1, X2 | 2 neurons | 1 neuron as Start Â» Separation line ||  Kein Gewinn durch komplizierter Modelle",,,,
,,,Â» Simple even linear model sufficient,,,,
,,,,,,,
10,16.Jan.,MuÃŸ um 9:20 stoppen um nach Zittau zu einem WS mit Infineon zu fahren,,,,,
,,,,,,,
,,Playground.TensorFlow.org     Repetition and continue,https://playground.tensorflow.org/,,,,
,,,,,,,
,,Spiral,,,,,
,,,Deep neural model required building a feature hierarchy,"Start with one hidden layer and two nodes, add nodes, add a hidden layer - see feature hierarchy emerging.",,,
,,,,3 hidden layers - be patient! - Model is improving. Optimization might start jumping around.,,,
,,,"6 layers, 5 layers w/ 8 nodes, last w/ 4 nodes, learning rate 0.03, ReLU",,,,
,,,,,,,
,,Application of Regression,,,,,
,>>>,Face Landmark Detection With End-To-End Regression in TensorFlow,https://medium.com/data-science/face-landmark-detection-with-cnns-tensorflow-cf4d191d2f0,,,,
,>>>,Face Images with Marked Landmark Points,Face Images with Marked Landmark Points,,,,
,,Colab:  Face Landmark Detection With TensorFlow,Face_Landmark_Detection.ipynb,,,,
,>>>,Colab: Face_Landmark_Detection - KtH 2023-04-19.ipynb,Face_Landmark_Detection - KtH 2025-12-17.ipynb,,Does not work on my face ...,,
,,,,,"Solutions: Image augmentation w/ zoom, rotation, shifts up/down & left & right.",,
,,,AI Models can only solve problems for which they were trained for!,,,,
,,,"If all faces in training where all in center and the face during test is not in center, the model does not know what to do and ...",,,,
,,,... guesses aka halucinates.,,,,
,,,"Solutions: Image augmentation w/ zoom, rotation, shifts up/down & left & right.",,,,
,Labor,Ihre Ergebnisse? ,,,,,
,>>>,Colab: Face_Landmark_Detection - KtH 2023-04-19.ipynb,Face_Landmark_Detection - KtH 2025-12-17.ipynb,,Does not work on my face ...,,
,,,,,,,
,,More human intelligence to make AI better,,,,,
,,SeparableConv2D vs Conv2D,One cause of speed of MobileNet ...,,,,
,>>>,A Basic Introduction to Separable Convolutions,A Basic Introduction to Separable Convolutions | by Chi-Feng Wang | TDS Archive | Medium,,,,
,,Using Depthwise Separable Convolutions in Tensorflow,Using Depthwise Separable Convolutions in Tensorflow - MachineLearningMastery.com,,,,
,,Why is this important?,MobileNet is fast! ,,,,
,>>>,On-edge TPU supports,Conv2d and DepthwiseConv2d,,,,
,,,TensorFlow models on the Edge TPU | Coral,,,,
,,,,,,,
,,More Hyperparameter,,,,,
,,,,,,,
,New in SS24,"Hyperparameters: Batch Size, Training Steps, Epochs & Learning Rate",,,,,
,New in SS24,Classification of Neural Network Hyperparameters,Classification of Neural Network Hyperparameters | Towards Data Science,,,,
,New in SS24,"All You Need to Know about Batch Size, Epochs and Training Steps in a Neural Network","All You Need to Know about Batch Size, Epochs and Training Steps in a Neural Network | by Rukshan Pramoditha | Data Science 365 | Medium",,,,
,,How to choose the Optimal Learning Rate for Neural Networks,How to Choose the Optimal Learning Rate for Neural Networks | Towards Data Science,,,,
,,Learning Rate Schedules and Decay in Keras Optimizers,Learning Rate Schedules and Decay in Keras Optimizers | by Rukshan Pramoditha | Data Science 365 | Medium,,,,
,,,,,,,
,,Regularization like Pruning for Decision Trees,,,,,
,,Classification of Neural Network Hyperparameters,https://towardsdatascience.com/classification-of-neural-network-hyperparameters-c7991b6937c3/#a105,,,,
,New in SS24,Regularization Methods for Neural Networks - Introduction,Regularization Methods for Neural Networks â€” Introduction | by Rukshan Pramoditha | Data Science 365 | Medium,,,,
,New in SS24,Using Early Stopping to reduce Overfitting ,Using Early Stopping to Reduce Overfitting in Neural Networks | by Rukshan Pramoditha | Data Science 365 | Medium,,,,
,New in SS24,Noise Regularization of Neural Networks,Noise Regularization of Neural Networks | by Rukshan Pramoditha | Data Science 365 | Medium,,"Noise avoids overfitting, since the training data and the weights and biases change",,
,New in SS24,How to apply L1 and L2 regularization to Keras Models,How to Apply L1 and L2 Regularization Techniques to Keras Models | by Rukshan Pramoditha | Data Science 365 | Medium,,,,
,New in SS24,How Dropout Regularization mitigates Overfitting in Neural Networks,How Dropout Regularization Mitigates Overfitting in Neural Networks | by Rukshan Pramoditha | Data Science 365 | Medium,,,,
,>>>,"overfit_and_underfit - Regularization L1, L2 & drop-out - KtH 2022-05-05","overfit_and_underfit - Regularization L1, L2 & drop-out - KtH 2025-12-17","Don't run CoLab, since data can not longer be downloaded from the original URL. BUT show and explain results of regularization.",,,
,,Web page about Colab,Overfit and underfit | TensorFlow Core,,,,
,,,,,,,
,,Model,Number of parameter,Overfit,,,
,,Tiny,481,none,,,
,,Small,753,none,,,
,,Medium,"10,241",yes,,,
,,Large,"803,329",yes,,,
,,,,,,,
,,L2 weight regularization plus dropout,Deliver the best performance on data not used for training,,,,
,,,,,,,
,Not SS22,Layers_normalizations - KtH 2021-04-14.ipynb,https://colab.research.google.com/drive/1HNmQeD1tOEqTMok0nL1UjGvwi5XeCnZq,,"Relevant, but confusing at this point in time.",,
,,,Normalization: Why?,,,,
,,,Values fit into smaller numbers of bits Â» Storage and speed,,,,
,,,"Â» Better converge, i.e. more stable training",,,,
,,,Â» Normalization aka Scaling for quantization,,,,
,,,Normalization with four values not only STD and MEAN,,,,
,,,Automatic denormalization Â» Cool!,,,,
11,23.Jan.,"Last lecture, since next week I am in Hoyerwerda.",,,,,
,,,,,,,
,,,,,,,
,,,"With the help of Gemini I fixed the Colab, please use the new version!",,,,
,,,It took some time to use Gemini in a Colab ... It took some time to get used to VScode too.,,,,
,,,BUT VScode makes me way more productive ...,,,,
,,,Gemini too ... for sure!,,,,
,,,,,,,
,,Repetition,,,,,
,,Learning rate schedule,,,,,
,,Higgs dataset has 28 features,"Need ""irrelevant features"" for overfitting!",,,,
,,Overfit and Underfit and Regularization using L2 weight sum and dropout of nodes.,"BTW Overfitting requires sufficient capacity AND irrelevant ""features"" to overfit too, e.g. details in an image.",,,,
,>>>,"overfit_and_underfit - Regularization L1, L2 & drop-out - KtH 2022-05-05","overfit_and_underfit - Regularization L1, L2 & drop-out - KtH 2026-01-21","Don't run CoLab, since training runs too long. Just explain the results in CoLab. Students can work on it at home. ",,,
,,,,,,,
,,Model,Number of parameter,Overfit,,,
,,Tiny,481,none,,,
,,Small,753,none,,,
,,Medium,"10,241",yes,,,
,,Large,"803,329",yes,,,
,,,,,,,
,,,,,,,
,,Repetition Hyperparameter,,,,,
,New in SS24,Classification of Neural Network Hyperparameters,https://towardsdatascience.com/classification-of-neural-network-hyperparameters-c7991b6937c3#a105,,,,
,,,,,,,
,,Repetition Regularization,,,,,
,New in SS24,Regularization Methods for Neural Networks - Introduction,https://medium.com/data-science-365/regularization-methods-for-neural-networks-introduction-326bce8077b3,,,,
,,,,,,,
,,,,,,,
,>>>,A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning,A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning | Towards Data Science,Validation Accuracy of the various versions to black board,,,
,,,Great figures and examples to explain transfer learning,,,,valAcc
,,,Go through it together and ...,,Simple CNN from scratch,,72%
,,,Study at home!,,CNN w/ regularization,,78%
,,,,,CNN w/ image augmentation,,82%
,,,,,Pre-trained VGG as feature extractor,,88%
,,,,,... w/ image augmentation,,90%
,,,,,... w/ fine-tuning the feature extractor,,96%
,,,,,,,
,Not in WS23,A Gentle Intro to Transfer Learning for Deep Learning,https://machinelearningmastery.com/transfer-learning-for-deep-learning/,Nothing new compared to article above,,,
,,Transfer learning and fine-tuning,Transfer learning and fine-tuning | TensorFlow Core,Webpage with CoLab,,,
,>>>,CoLab: Transfer Learning and fine-tuning - KtH  2022-05-08,Transfer Learning and fine-tuning - KtH  2022-10-18,,,,
,,,,,,,
,,Transfer Learning with TensorFlow Hub,https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub,,,,
,>>>,transfer_learning_with_hub - KtH 2022-11-02.ipynb,https://colab.research.google.com/drive/1cFBQkKGuhk_RLHsNRtmx3QEhikJS35DD,,,,
,,TensorFlow Hub image classification models for TF2,https://tfhub.dev/google/imagenet/mobilenet_v1_100_192/classification/5,"MobileNet with depth multiplier, i.e. number of features in Conv. layers",,,
,,,,,,,
,,Repetition of entire term,What did you learn?,,,,
,,,History of AI / ML,,,,
,,,Start: Dartmouth Worshop in 1956,,,,
,,,Expert systems and Neural Networks,,,,
,,,Expert Systems and traditional ML are ubiquitous,,,,
,,,Neural Networks: First success recognizing digits with DNN and CNN under Jan LeCun 1989,,,,
,,,Teachable Machines,,,,
,,,Image Classification,,,,
,,,Pose Regression,,,,
,,,Audio Classification,,,,
,,,Dancing with AI - Integration into interactive systems,,,,
,,,"Hand, finger, face, body sensing ...",,,,
,,,Tensorflow / Keras: Simple regression,,,,
,,,Image classification with Fashion MNIST,,,,
,,,Probabilities and Softmax normalisation of Logits,,,,
,,,"Loss function: Cross Entropy, Information Content, Claude Shannon",,,,
,,,ML on edge devices,,,,
,,,Lab experiments: Accuracy and Efficiency vs Capacity of model,,,,
,,,Competition: Optimization of number of nodes and layers of a DNN for Fashion MNIST,,,,
,,,Maximal Efficiency at minimal accuracy Â» Drastic increase in efficiency ,,,,
,,,Convolutional Neural Networks,,,,
,,,Manually discovered kernels like Sobel for edge detection,,,,
,,,Accuracy and Efficiency  ,,,,
,,,CNN is better than DNN,,,,
,,,Jan LeCun's MNIST solution in DNN and CNN in one lecture,,,,
,,,Thanks to frameworks enabling working on higher levels of abstraction,,,,
,,,Python Â» Tensorflow Â» Keras Â» Teachable Machines,,,,
,,,Tensorboard to monitor long training runs,,,,
,,,Regression for MPG,,,,
,,,Multiple features to multiple layers Â» Piecewise linear approximation,,,,
,,,Activation functions as a Hyperparameter,,,,
,,,x * sin(x) Example to study ,,,,
,,,"Accuracy vs complexity of data, capacity of model, amount of training data, noise in training data",,,,
,,,Quantization for on-the-edge AI,,,,
,,,Playground.TensorFlow.org to see why Deep Learning can solve all problems ...,,,,
,,,... using piece linear approximation,,,,
,,,Face Landmark Regression Â» AI does what it was trained for ,,,,
,,,Regularization methods to avoid overfitting,,,,
,,,"L1, L2, drop-out, Early Stopping",,,,
,,,Transfer learning with fine-tuning,,,,
,,,Pre-trained AI as feature selector,,,,
,,,,,,,
,,Final question: ,What is Teachable Machines doing? .. in the browser?,,,,
,,,,,,,
,,Feedback on lecture,Good?,,,,
,,,"Einstieg mit Teachable Machines gut, aber weniger um am Ende mehr Zeit.",,,,
,,,,,,,
,,,Room for improvement?,,,,
,,,LLM ...,,,,
,,,,,,,
,,,,,,,
,,MS DM1 - More Deep Learning ,Is there more?  ;-),,,,
,,,Data Augmentation,,,,
,,,... somewhat boring but makes the difference,,,,
,,,EfficientNet,,,,
,,,... what you did in the lab ... ,,,,
,,,Object Detection,,,,
,,,... a whole new class of models,,,,
,,,Segmentation using e.g. UNET,,,,
,,,Auto Encoder to anomaly detection,,,,
,,,Adversarial training ,,,,
,,,... best image to foul a model.,,,,
,,,Generative AI,,,,
,,,"... generating text, images and both.",,,,
,,,Explainable AI,,,,
,,,... AI can explain itself. A little bit at least.,,,,
,,,Sequence Prediction w/ Recurrent Models like LSTM,,,,
,,,Transformer,,,,
,,,,,,,
,,Consultations to focus on your projects,,,,,
,,,,,,,
,,,,,,,
,End of Lectures in WS25,,,,,,